{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = {}\n",
    "\n",
    "for word in words:\n",
    "    chs = [\"<S>\"] + list(word) + [\"<E>\"]\n",
    "    for char1, char2 in zip(chs, chs[1:]):\n",
    "        bigram = (char1, char2)\n",
    "        b[bigram] = b.get(bigram, 0) + 1\n",
    "        # print(char1, char2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(b.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing bigrams as a 2D Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.zeros((3, 5), dtype=torch.int32)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((27, 27), dtype=torch.int32)\n",
    "\n",
    "chars = [\".\"] + sorted(list(set(\"\".join(words))))\n",
    "s_to_i = {s: i for i, s in enumerate(chars)}\n",
    "\n",
    "s_to_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    chs = [\".\"] + list(word) + [\".\"]\n",
    "    for char1, char2 in zip(chs, chs[1:]):\n",
    "        bigram = (char1, char2)\n",
    "        i, j = s_to_i[char1], s_to_i[char2]\n",
    "        N[i, j] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualizing the bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_to_s = {i: s for s, i in s_to_i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "plt.imshow(N, cmap=\"Blues\")\n",
    "\n",
    "for i in range(len(chars)):\n",
    "    for j in range(len(chars)):\n",
    "        char_string = i_to_s[i] + i_to_s[j]\n",
    "        plt.text(j, i, char_string, ha=\"center\", va=\"bottom\", color=\"gray\")\n",
    "        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color=\"gray\")\n",
    "\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the bigram language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generating it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = N[0].float()\n",
    "p = p / p.sum()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "ix = torch.multinomial(p, 1, generator=g).item()\n",
    "i_to_s[ix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probablity matrix\n",
    "# P_ij = N_ij / sum(N_i)\n",
    "P = (N + 1).float()\n",
    "P /= P.sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping over the generation process\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "for i in range(5):\n",
    "    ix = 0\n",
    "    generated = [i_to_s[ix]]\n",
    "    while True:\n",
    "        # Get the row of the current character and calculate the probabilities\n",
    "        p = P[ix]\n",
    "        # p = N[ix].float()\n",
    "        # p = p / p.sum()\n",
    "\n",
    "        # If the model were completely random, the probability distribution would be uniform over the set of defined characters\n",
    "        # p = torch.ones(27) / 27.0\n",
    "\n",
    "        # Sample the next character\n",
    "        ix = torch.multinomial(p, 1, replacement=True, generator=g).item()\n",
    "        generated.append(i_to_s[ix])\n",
    "\n",
    "        # Stop if special character is sampled\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(\"\".join(generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measuring performance using log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "for word in words:\n",
    "    chs = [\".\"] + list(word) + [\".\"]\n",
    "    for char1, char2 in zip(chs, chs[1:]):\n",
    "        bigram = (char1, char2)\n",
    "        i, j = s_to_i[char1], s_to_i[char2]\n",
    "        prob = P[i, j]\n",
    "        logprob = torch.log(prob)\n",
    "\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "\n",
    "        # print(f\"{char1}{char2}, {prob:.4f} {logprob:.4f}\")\n",
    "\n",
    "print(f\"{log_likelihood=}\")\n",
    "print(f\"NLL: {-log_likelihood=}\")\n",
    "print(f\"Average NLL: {-log_likelihood/n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the training set of bigrams (x, y)\n",
    "character_set = [\".\"]\n",
    "character_set.extend(sorted(list(set(\"\".join(words)))))\n",
    "\n",
    "s_to_i = {s: i for i, s in enumerate(character_set)}\n",
    "\n",
    "dims = len(character_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [], []\n",
    "\n",
    "for word in words[:1]:\n",
    "    # Adding start and end tokens to correctly capture all bigrams\n",
    "    chs = ['.'] + list(word) + ['.']\n",
    "    for c1, c2 in zip(chs, chs[1:]):\n",
    "        print(c1, c2)\n",
    "        i, j = s_to_i[c1], s_to_i[c2]\n",
    "        xs.append(i)\n",
    "        ys.append(j)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding of the input vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\Projects\\nn-zero-to-hero\\makemore\\lec2.ipynb Cell 33\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Projects/nn-zero-to-hero/makemore/lec2.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Projects/nn-zero-to-hero/makemore/lec2.ipynb#X44sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m x_enc \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mone_hot(xs, num_classes\u001b[39m=\u001b[39mdims)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Projects/nn-zero-to-hero/makemore/lec2.ipynb#X44sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(x_enc)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Projects/nn-zero-to-hero/makemore/lec2.ipynb#X44sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m x_enc\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "x_enc = F.one_hot(xs, num_classes=dims).float()\n",
    "\n",
    "plt.imshow(x_enc)\n",
    "x_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One layer of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a matrix of weights (d x d), setting all initial values randomly\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((dims, dims), generator=g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we generate the weight matrix, we feed forward the input vector to the neural network and get the output vector. The steps to do this are as follows:\n",
    "1. Multiply the input vector with the weight matrix to get the logits\n",
    "2. Exponentiate the logits to get the log counts\n",
    "3. Normalize the log counts to get the probabilities\n",
    "\n",
    "> Steps 2 and 3 above correspond to the `softmax` function.\n",
    "\n",
    "Doing this gives us a vector of probabilities for each input vector. This helps us predict the next token of the bigram. \n",
    "\n",
    "Then, we calculate the loss by optimizing the predicted probability of the actual token, and use backpropagation to optimize the loss function by tuning the weights of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forward pass\n",
    "x_enc = F.one_hot(xs, num_classes=dims).float()\n",
    "logits = (\n",
    "    x_enc @ W\n",
    ")  # logits_ij refers to the firing rate of the j-th character given the i-th character\n",
    "\n",
    "counts = logits.exp()  # log-counts, equivalent to the Tensor `N` defined above.\n",
    "probs = counts / counts.sum(\n",
    "    dim=1, keepdim=True\n",
    ")  # probabilities, equivalent to the Tensor `P` defined above.\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlls = torch.zeros(len(xs))\n",
    "\n",
    "for i in range(len(xs)):\n",
    "    x = xs[i].item()\n",
    "    y = ys[i].item()\n",
    "\n",
    "    print(\"-\"*10)\n",
    "    print(f'Bigram example {i+1}: \"{i_to_s[x]}{i_to_s[y]}\", {x=}, {y=}')\n",
    "    print(f'Input to NN: {x}')\n",
    "    print(f'Output probabilities: {probs[i]}')\n",
    "    print(f'Actual next character: {y}')\n",
    "    p = probs[i, y]\n",
    "    print(f'Probability assigned to actual next character: {p}')\n",
    "    logp = torch.log(p)\n",
    "    print(f'Log-probability assigned to actual next character: {logp}')\n",
    "    nll = -logp\n",
    "    print(f'NLL: {nll}')\n",
    "\n",
    "    nlls[i] = nll\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(f'Average NLL: ', nlls.mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Manual optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a matrix of weights (d x d), setting all initial values randomly\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((dims, dims), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forward pass\n",
    "x_enc = F.one_hot(xs, num_classes=dims).float()\n",
    "logits = x_enc @ W\n",
    "\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(dim=1, keepdim=True)\n",
    "\n",
    "## Calculate loss\n",
    "# probs[i, j] refers to the probability of the j-th character given the i-th character\n",
    "# This is the probability assigned by the model that we want to maximize\n",
    "loss = probs[torch.arange(len(xs)), ys].log().neg().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Backward pass\n",
    "W.grad = None\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update weights in the opposite direction of the gradient\n",
    "learning_rate = 1e-1\n",
    "W.data += -learning_rate * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the training set of bigrams (x, y)\n",
    "character_set = [\".\"]\n",
    "character_set.extend(sorted(list(set(\"\".join(words)))))\n",
    "\n",
    "s_to_i = {s: i for i, s in enumerate(character_set)}\n",
    "i_to_s = {i: s for s, i in s_to_i.items()}\n",
    "\n",
    "dims = len(character_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 228146\n"
     ]
    }
   ],
   "source": [
    "xs, ys = [], []\n",
    "\n",
    "for word in words:\n",
    "    chs = [\".\"] + list(word) + [\".\"]\n",
    "    for c1, c2 in zip(chs, chs[1:]):\n",
    "        # print(c1, c2)\n",
    "        i, j = s_to_i[c1], s_to_i[c2]\n",
    "        xs.append(i)\n",
    "        ys.append(j)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num_examples = xs.nelement()\n",
    "print(f\"Number of examples: {num_examples}\")\n",
    "\n",
    "# Initialize the network\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((dims, dims), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 27])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape\n",
    "F.one_hot(xs, num_classes=dims).float().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 50\n",
    "\n",
    "for k in range(200):\n",
    "    x_enc = F.one_hot(xs, num_classes=dims).float()\n",
    "    logits = x_enc @ W\n",
    "\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(dim=1, keepdim=True)\n",
    "\n",
    "    loss = (\n",
    "        probs[torch.arange(num_examples), ys].log().neg().mean()\n",
    "        + 1e-2 * (W**2).mean()\n",
    "    )\n",
    "\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    W.data += -learning_rate * W.grad\n",
    "\n",
    "    print(f\"Iteration {k}: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the next character\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "    ix = 0\n",
    "    out = [i_to_s[ix]]\n",
    "    \n",
    "    while True:\n",
    "        # print(f\"Input character: {x}\")\n",
    "        # print(f\"Probabilities: {probs[x]}\")\n",
    "\n",
    "        x_enc = F.one_hot(torch.tensor([ix]), num_classes=dims).float()\n",
    "        logits = x_enc @ W\n",
    "\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(dim=1, keepdim=True)\n",
    "\n",
    "        ix = torch.multinomial(\n",
    "            probs, 1, replacement=True, generator=g\n",
    "        ).item()\n",
    "        out.append(i_to_s[ix])\n",
    "\n",
    "        # print(f\"Predicted next character: {next_char}\")\n",
    "\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(\"=\" * 100)\n",
    "    print(\"\".join(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
